{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering: Encoding, Scaling, and Feature Creation\n",
        "\n",
        "This notebook demonstrates comprehensive feature engineering techniques and their impact on model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "with open('../adult_data.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "X_train = data['X_train']\n",
        "X_test = data['X_test']\n",
        "y_train = data['y_train']\n",
        "y_test = data['y_test']\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering Pipeline\n",
        "\n",
        "We'll apply:\n",
        "1. **One-Hot Encoding**: Better than label encoding for categorical variables\n",
        "2. **Feature Scaling**: Standardize numerical features\n",
        "3. **Feature Creation**: Create interaction features\n",
        "4. **Feature Selection**: Select most important features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify column types\n",
        "numerical_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"Numerical columns: {numerical_cols}\")\n",
        "print(f\"Categorical columns: {categorical_cols}\")\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Fit and transform\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"\\nAfter preprocessing:\")\n",
        "print(f\"Training shape: {X_train_processed.shape}\")\n",
        "print(f\"Test shape: {X_test_processed.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Creation: Interaction Features\n",
        "\n",
        "Let's create some interaction features that might be informative.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create interaction features from original data\n",
        "# Age * Hours per week (work intensity)\n",
        "X_train['age_hours'] = X_train['age'] * X_train['hours-per-week']\n",
        "X_test['age_hours'] = X_test['age'] * X_test['hours-per-week']\n",
        "\n",
        "# Capital gain - capital loss (net capital)\n",
        "X_train['net_capital'] = X_train['capital-gain'] - X_train['capital-loss']\n",
        "X_test['net_capital'] = X_test['capital-gain'] - X_test['capital-loss']\n",
        "\n",
        "# Education * Hours (education-work interaction)\n",
        "X_train['edu_hours'] = X_train['education-num'] * X_train['hours-per-week']\n",
        "X_test['edu_hours'] = X_test['education-num'] * X_test['hours-per-week']\n",
        "\n",
        "# Add new numerical columns to the list\n",
        "new_numerical_cols = numerical_cols + ['age_hours', 'net_capital', 'edu_hours']\n",
        "\n",
        "# Recreate preprocessor with new features\n",
        "preprocessor_enhanced = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), new_numerical_cols),\n",
        "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "X_train_enhanced = preprocessor_enhanced.fit_transform(X_train)\n",
        "X_test_enhanced = preprocessor_enhanced.transform(X_test)\n",
        "\n",
        "print(f\"Enhanced features shape: {X_train_enhanced.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Selection\n",
        "\n",
        "Select the most important features using statistical tests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature selection\n",
        "selector = SelectKBest(score_func=f_classif, k=50)  # Select top 50 features\n",
        "X_train_selected = selector.fit_transform(X_train_enhanced, y_train)\n",
        "X_test_selected = selector.transform(X_test_enhanced)\n",
        "\n",
        "print(f\"After feature selection:\")\n",
        "print(f\"Training shape: {X_train_selected.shape}\")\n",
        "print(f\"Test shape: {X_test_selected.shape}\")\n",
        "\n",
        "# Get feature scores\n",
        "feature_scores = selector.scores_\n",
        "top_features_idx = np.argsort(feature_scores)[-20:][::-1]\n",
        "print(f\"\\nTop 20 feature scores:\")\n",
        "for idx in top_features_idx:\n",
        "    print(f\"  Feature {idx}: {feature_scores[idx]:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Models with Engineered Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 1: Basic preprocessing (one-hot + scaling)\n",
        "model1 = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model1.fit(X_train_processed, y_train)\n",
        "y_pred1 = model1.predict(X_test_processed)\n",
        "acc1 = accuracy_score(y_test, y_pred1)\n",
        "\n",
        "# Model 2: With interaction features\n",
        "model2 = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model2.fit(X_train_enhanced, y_train)\n",
        "y_pred2 = model2.predict(X_test_enhanced)\n",
        "acc2 = accuracy_score(y_test, y_pred2)\n",
        "\n",
        "# Model 3: With feature selection\n",
        "model3 = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model3.fit(X_train_selected, y_train)\n",
        "y_pred3 = model3.predict(X_test_selected)\n",
        "acc3 = accuracy_score(y_test, y_pred3)\n",
        "\n",
        "print(\"Model Performance Comparison:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Model 1 (One-Hot + Scaling):        {acc1:.4f}\")\n",
        "print(f\"Model 2 (+ Interaction Features):   {acc2:.4f}\")\n",
        "print(f\"Model 3 (+ Feature Selection):     {acc3:.4f}\")\n",
        "\n",
        "# Load baseline for comparison\n",
        "with open('../baseline_results.pkl', 'rb') as f:\n",
        "    baseline = pickle.load(f)\n",
        "\n",
        "print(f\"\\nBaseline (Label Encoding only):    {baseline['accuracy']:.4f}\")\n",
        "print(f\"\\nImprovement over baseline:\")\n",
        "print(f\"  Model 1: {acc1 - baseline['accuracy']:.4f}\")\n",
        "print(f\"  Model 2: {acc2 - baseline['accuracy']:.4f}\")\n",
        "print(f\"  Model 3: {acc3 - baseline['accuracy']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "models = ['Baseline', 'One-Hot+Scaling', '+Interactions', '+Selection']\n",
        "accuracies = [baseline['accuracy'], acc1, acc2, acc3]\n",
        "\n",
        "axes[0].bar(models, accuracies, color=['gray', 'steelblue', 'coral', 'green'])\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Model Performance Comparison')\n",
        "axes[0].set_ylim([0.8, 0.9])\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(accuracies):\n",
        "    axes[0].text(i, v + 0.002, f'{v:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# Confusion matrix for best model\n",
        "best_model_idx = np.argmax(accuracies[1:]) + 1\n",
        "best_pred = [y_pred1, y_pred2, y_pred3][best_model_idx - 1]\n",
        "cm = confusion_matrix(y_test, best_pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
        "            xticklabels=['≤50K', '>50K'],\n",
        "            yticklabels=['≤50K', '>50K'])\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "axes[1].set_title(f'Best Model Confusion Matrix\\n(Model {best_model_idx}, Acc: {accuracies[best_model_idx]:.4f})')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance from best model\n",
        "best_model = [model1, model2, model3][best_model_idx - 1]\n",
        "feature_importance = np.abs(best_model.coef_[0])\n",
        "top_20_idx = np.argsort(feature_importance)[-20:][::-1]\n",
        "\n",
        "print(\"Top 20 Most Important Features:\")\n",
        "for idx in top_20_idx:\n",
        "    print(f\"  Feature {idx}: {feature_importance[idx]:.4f}\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(range(20), feature_importance[top_20_idx])\n",
        "plt.yticks(range(20), [f'Feature {i}' for i in top_20_idx])\n",
        "plt.xlabel('Feature Importance (|Coefficient|)')\n",
        "plt.title('Top 20 Feature Importances')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'baseline_accuracy': baseline['accuracy'],\n",
        "    'model1_accuracy': acc1,\n",
        "    'model2_accuracy': acc2,\n",
        "    'model3_accuracy': acc3,\n",
        "    'best_model': best_model,\n",
        "    'preprocessor': preprocessor_enhanced if best_model_idx == 2 else preprocessor,\n",
        "    'selector': selector if best_model_idx == 3 else None\n",
        "}\n",
        "\n",
        "with open('../feature_engineering_results.pkl', 'wb') as f:\n",
        "    pickle.dump(results, f)\n",
        "\n",
        "print(\"\\nResults saved to '../feature_engineering_results.pkl'\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
